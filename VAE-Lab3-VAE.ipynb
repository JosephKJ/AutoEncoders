{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE-Experiment-3:\n",
    "\n",
    "\n",
    "#### Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Autoencoders (VAE) are generative models which use deep feed-forward neural networks to model complicated probability density functions $p(x)$. As discussed in the first notebook, a VAE models a complicated probability distribution as a complex deterministic transformation of a simple probability distribution. A $VAE$ model can be trained to fit a data distribution by maximizing the log-likelihood of the samples from the distribution. Specifically, as discussed in the second notebook, VAE actually maximizes a variational lower-bound to the log-likelihood. It simultaneously learns an encoder network in order to estimate the variational lower-bound during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's learn a simple fully-connected Variational Autoencoder that generates digits similar to the ones in the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Import necessary modules\n",
    "##########################\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "# Set parameters\n",
    "################\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "seed = 1\n",
    "\n",
    "n_classes = 10\n",
    "z_dim = 2\n",
    "X_dim = 784\n",
    "train_batch_size = 100\n",
    "valid_batch_size = train_batch_size\n",
    "N = 1000\n",
    "epochs = 5\n",
    "\n",
    "params = {}\n",
    "params['cuda'] = cuda\n",
    "params['n_classes'] = n_classes\n",
    "params['z_dim'] = z_dim\n",
    "params['X_dim'] = X_dim\n",
    "params['train_batch_size'] = train_batch_size\n",
    "params['valid_batch_size'] = valid_batch_size\n",
    "params['N'] = N\n",
    "params['epochs'] = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# Load data and create Data loaders\n",
    "###################################\n",
    "\n",
    "def load_data(data_path='./data/VAE/'):\n",
    "    print('loading data!')\n",
    "    trainset_labeled = pickle.load(open(data_path + \"train_labeled.p\", \"rb\"))\n",
    "    trainset_unlabeled = pickle.load(open(data_path + \"train_unlabeled.p\", \"rb\"))\n",
    "    # Set -1 as labels for unlabeled data\n",
    "    trainset_unlabeled.train_labels = torch.from_numpy(np.array([-1] * 47000))\n",
    "    validset = pickle.load(open(data_path + \"validation.p\", \"rb\"))\n",
    "\n",
    "    train_labeled_loader = torch.utils.data.DataLoader(trainset_labeled,\n",
    "                                                       batch_size=train_batch_size,\n",
    "                                                       shuffle=True, **kwargs)\n",
    "\n",
    "    train_unlabeled_loader = torch.utils.data.DataLoader(trainset_unlabeled,\n",
    "                                                         batch_size=train_batch_size,\n",
    "                                                         shuffle=True, **kwargs)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(validset, batch_size=valid_batch_size, shuffle=True)\n",
    "\n",
    "    return train_labeled_loader, train_unlabeled_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Define Networks\n",
    "#################\n",
    "\n",
    "# Encoder\n",
    "class Q_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q_net, self).__init__()\n",
    "        self.lin1 = nn.Linear(X_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        # Gaussian code (z)\n",
    "        self.lin3gauss_mean = nn.Linear(N, z_dim)\n",
    "        self.lin3gauss_logvar = nn.Linear(N, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        xgauss_mean = self.lin3gauss_mean(x)\n",
    "        xgauss_logvar = self.lin3gauss_logvar(x)\n",
    "\n",
    "        return xgauss_mean, xgauss_logvar\n",
    "\n",
    "\n",
    "# Decoder\n",
    "class P_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(P_net, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        self.lin3 = nn.Linear(N, X_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# Utility functions\n",
    "###################\n",
    "\n",
    "\n",
    "def save_model(model, filename):\n",
    "    print('Best model so far, saving it...')\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "\n",
    "def report_loss(epoch, recon_loss):\n",
    "    '''\n",
    "    Print loss\n",
    "    '''\n",
    "    print('Epoch-{}; recon_loss: {:.4}'.format(epoch,recon_loss.data[0]))\n",
    "\n",
    "\n",
    "def create_latent(Q, loader):\n",
    "    '''\n",
    "    Creates the latent representation for the samples in loader\n",
    "    return:\n",
    "        z_values: numpy array with the latent representations\n",
    "        labels: the labels corresponding to the latent representations\n",
    "    '''\n",
    "    Q.eval()\n",
    "    labels = []\n",
    "\n",
    "    for batch_idx, (X, target) in enumerate(loader):\n",
    "\n",
    "        X = X * 0.3081 + 0.1307\n",
    "        # X.resize_(loader.batch_size, X_dim)\n",
    "        X, target = Variable(X), Variable(target)\n",
    "        labels.extend(target.data.tolist())\n",
    "        if cuda:\n",
    "            X, target = X.cuda(), target.cuda()\n",
    "        # Reconstruction phase\n",
    "        z_sample = Q(X)\n",
    "        if batch_idx > 0:\n",
    "            z_values = np.concatenate((z_values, np.array(z_sample.data.tolist())))\n",
    "        else:\n",
    "            z_values = np.array(z_sample.data.tolist())\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return z_values, labels\n",
    "\n",
    "\n",
    "def get_X_batch(data_loader, params, size=None):\n",
    "    if size is None:\n",
    "        size = data_loader.batch_size\n",
    "\n",
    "    data_loader.batch_size = size\n",
    "\n",
    "    for X, target in data_loader:\n",
    "        break\n",
    "\n",
    "    train_batch_size = params['train_batch_size']\n",
    "    X_dim = params['X_dim']\n",
    "    cuda = params['cuda']\n",
    "\n",
    "    X = X * 0.3081 + 0.1307\n",
    "\n",
    "    X = X[:size]\n",
    "    target = target[:size]\n",
    "\n",
    "    X.resize_(size, X_dim)\n",
    "    X, target = Variable(X), Variable(target)\n",
    "\n",
    "    if cuda:\n",
    "        X, target = X.cuda(), target.cuda()\n",
    "\n",
    "    return X, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE objective function and the reparameterization trick that allows us to back-propagate gradient through the sampling process in the latent layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Training procedure\n",
    "####################\n",
    "\n",
    "def train(P, Q, P_decoder, Q_encoder, data_loader):\n",
    "    '''\n",
    "    Train procedure for one epoch.\n",
    "    '''\n",
    "    TINY = 1e-15\n",
    "    # Set the networks in train mode (apply dropout when needed)\n",
    "    Q.train()\n",
    "    P.train()\n",
    "\n",
    "    # Loop through the labeled and unlabeled dataset getting one batch of samples from each\n",
    "    # The batch size has to be a divisor of the size of the dataset or it will return\n",
    "    # invalid samples\n",
    "    for X, target in data_loader:\n",
    "\n",
    "        # Load batch and normalize samples to be between 0 and 1\n",
    "        X = X * 0.3081 + 0.1307\n",
    "        X.resize_(train_batch_size, X_dim)\n",
    "        X, target = Variable(X), Variable(target)\n",
    "        if cuda:\n",
    "            X, target = X.cuda(), target.cuda()\n",
    "\n",
    "        # Init gradients\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "\n",
    "\n",
    "        # Reconstruction phase\n",
    "\n",
    "        z_mean, z_logvar = Q(X)\n",
    "        std = z_logvar.mul(0.5).exp_()\n",
    "        if cuda:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        z_sample = eps.mul(std).add_(z_mean)\n",
    "\n",
    "        X_sample = P(z_sample)\n",
    "        criterion = nn.BCELoss()\n",
    "        criterion.size_average = False\n",
    "        recon_loss = criterion(X_sample, X.resize(train_batch_size, X_dim))\n",
    "\n",
    "        # -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD_element = z_mean.pow(2).add_(z_logvar.exp()).mul_(-1).add_(1).add_(z_logvar)\n",
    "        KLD_loss = torch.sum(KLD_element).mul_(-0.5)\n",
    "\n",
    "        loss =  recon_loss + KLD_loss\n",
    "\n",
    "        loss.backward()\n",
    "        P_decoder.step()\n",
    "        Q_encoder.step()\n",
    "\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "\n",
    "    return recon_loss\n",
    "\n",
    "\n",
    "def generate_model(train_labeled_loader, train_unlabeled_loader, valid_loader):\n",
    "    torch.manual_seed(10)\n",
    "\n",
    "    if cuda:\n",
    "        Q = Q_net().cuda()\n",
    "        P = P_net().cuda()\n",
    "    else:\n",
    "        Q = Q_net()\n",
    "        P = P_net()\n",
    "\n",
    "    # Set learning rates\n",
    "    gen_lr = 0.0005\n",
    "\n",
    "    # Set optimizators\n",
    "    P_decoder = optim.Adam(P.parameters(), lr=gen_lr)\n",
    "    Q_encoder = optim.Adam(Q.parameters(), lr=gen_lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        recon_loss = train(P, Q, P_decoder, Q_encoder,\n",
    "                                                 train_unlabeled_loader)\n",
    "        if epoch % 1 == 0:\n",
    "            report_loss(epoch, recon_loss)\n",
    "\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data!\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "3000\n",
      "750\n",
      "Epoch-0; recon_loss: 1.535e+04\n",
      "Epoch-1; recon_loss: 1.608e+04\n",
      "Epoch-2; recon_loss: 1.544e+04\n",
      "Epoch-3; recon_loss: 1.497e+04\n",
      "Epoch-4; recon_loss: 1.446e+04\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Train a generative model\n",
    "##########################\n",
    "\n",
    "train_labeled_loader, train_unlabeled_loader, valid_loader = load_data()\n",
    "Q, P = generate_model(train_labeled_loader, train_unlabeled_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Save trained model\n",
    "####################\n",
    "\n",
    "# Save trained model\n",
    "torch.save(Q,'./data/VAE/TrainedModels/VAE_mytraining_Q.pt')\n",
    "torch.save(P,'./data/VAE/TrainedModels/VAE_mytraining_P.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3526a2874d2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load model trained for 200 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mQ_pt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/VAE/TrainedModels/VAE_preTrained_Q.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mP_pt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/VAE/TrainedModels/VAE_preTrained_P.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephkj/anaconda2/lib/python2.7/site-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephkj/anaconda2/lib/python2.7/site-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephkj/anaconda2/lib/python2.7/site-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mroot_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 deserialized_objects[root_key] = restore_location(\n\u001b[0;32m--> 348\u001b[0;31m                     data_type(size), location)\n\u001b[0m\u001b[1;32m    349\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephkj/anaconda2/lib/python2.7/site-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephkj/anaconda2/lib/python2.7/site-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mdevice_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephkj/anaconda2/lib/python2.7/site-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephkj/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_idx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephkj/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m         raise RuntimeError(\n\u001b[1;32m     83\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_sparse_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephkj/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Load trained model\n",
    "####################\n",
    "\n",
    "# Load model trained for 200 epochs\n",
    "Q_pt = torch.load('./data/VAE/TrainedModels/VAE_preTrained_Q.pt')\n",
    "P_pt = torch.load('./data/VAE/TrainedModels/VAE_preTrained_P.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Visualize reconstruction\n",
    "##########################\n",
    "\n",
    "def create_reconstruction(Q, P, data_loader, params):\n",
    "    Q.eval()\n",
    "    P.eval()\n",
    "    X, label = get_X_batch(data_loader, params, size=1)\n",
    "\n",
    "\n",
    "    ## Sampling from latent distribution\n",
    "\n",
    "    z_mean, z_logvar = Q(X)\n",
    "    std = z_logvar.mul(0.5).exp_()\n",
    "    if cuda:\n",
    "       eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "    else:\n",
    "       eps = torch.FloatTensor(std.size()).normal_()\n",
    "    eps = Variable(eps)\n",
    "    z_sample = eps.mul(std).add_(z_mean)\n",
    "\n",
    "\n",
    "    ## Forwarding the mean of the latent distribution\n",
    "\n",
    "    #z_mean, z_logvar = Q(X)\n",
    "    #z_sample = z_mean\n",
    "\n",
    "    x = P(z_sample)\n",
    "\n",
    "    img_orig = np.array(X[0].data.tolist()).reshape(28, 28)\n",
    "    img_rec = np.array(x[0].data.tolist()).reshape(28, 28)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_orig)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img_rec)\n",
    "\n",
    "\n",
    "data_loader = valid_loader    # Training data:  train_unlabeled_loader  |  Validation data:  valid_loader\n",
    "\n",
    "create_reconstruction(Q_pt, P_pt, data_loader, params)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Visualize generation\n",
    "######################\n",
    "\n",
    "def grid_plot2d(Q, P, params):\n",
    "    Q.eval()\n",
    "    P.eval()\n",
    "\n",
    "    cuda = params['cuda']\n",
    "\n",
    "    z1 = Variable(torch.from_numpy(np.arange(-1, 1, 0.15).astype('float32')))\n",
    "    z2 = Variable(torch.from_numpy(np.arange(-1, 1, 0.15).astype('float32')))\n",
    "\n",
    "    if cuda:\n",
    "        z1, z2 = z1.cuda(), z2.cuda()\n",
    "\n",
    "    nx, ny = len(z1), len(z2)\n",
    "    plt.subplot()\n",
    "    gs = gridspec.GridSpec(nx, ny, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i, g in enumerate(gs):\n",
    "        z = torch.cat((z1[i / ny], z2[i % nx])).resize(1, 2)\n",
    "        x = P(z)\n",
    "\n",
    "        ax = plt.subplot(g)\n",
    "        img = np.array(x.data.tolist()).reshape(28, 28)\n",
    "        ax.imshow(img, )\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_aspect('auto')\n",
    "\n",
    "\n",
    "grid_plot2d(Q_pt, P_pt, params)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# Visualize latent distribution\n",
    "###############################\n",
    "\n",
    "Q_pt.eval()\n",
    "P_pt.eval()\n",
    "\n",
    "X, label = get_X_batch(data_loader, params, size=10000)\n",
    "z_mean, z_logvar = Q_pt(X)\n",
    "\n",
    "z_mean = np.array(z_mean.data.tolist())\n",
    "label = np.array(label.data.tolist())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(z_mean[:,0], z_mean[:,1], c=label, cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-shaped distribution\n",
    "\n",
    "Now, remember the C-shaped distribution from the first notebook? Here it is again, Can you learn a Variational Autoencoder that models this distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Reset workspace for next experiment\n",
    "#####################################\n",
    "\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Import necessary modules\n",
    "##########################\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "# Set parameters\n",
    "################\n",
    "\n",
    "cuda = True\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "seed = 1\n",
    "\n",
    "z_dim = 2\n",
    "X_dim = 2\n",
    "train_batch_size = 100\n",
    "valid_batch_size = train_batch_size\n",
    "N = 100\n",
    "epochs = 100\n",
    "\n",
    "params = {}\n",
    "params['cuda'] = cuda\n",
    "params['z_dim'] = z_dim\n",
    "params['X_dim'] = X_dim\n",
    "params['train_batch_size'] = train_batch_size\n",
    "params['valid_batch_size'] = valid_batch_size\n",
    "params['N'] = N\n",
    "params['epochs'] = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Define Data-loader\n",
    "####################\n",
    "\n",
    "def data_loader_Cdist(numSamples):\n",
    "\n",
    "    z = np.random.randn(numSamples,2).astype(np.float32)        # Sample from Gaussian distribution\n",
    "    z1 = z[:,0]\n",
    "    z2 = z[:,1]\n",
    "    os = 10\n",
    "    ost = np.pi/2\n",
    "    x1 = -(1.5*os+z1)*(np.sin(z2+ost))\n",
    "    x2 = (os+z1)*np.cos(z2+ost)\n",
    "\n",
    "    x = np.concatenate([np.expand_dims(x1,1),np.expand_dims(x2,1)],1)\n",
    "    dummy = np.zeros(len(x))\n",
    "\n",
    "    return torch.from_numpy(x), torch.from_numpy(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Define Networks\n",
    "#################\n",
    "\n",
    "# Encoder\n",
    "class Q_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q_net, self).__init__()\n",
    "        self.lin1 = nn.Linear(X_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        # Gaussian code (z)\n",
    "        self.lin3gauss_mean = nn.Linear(N, z_dim)\n",
    "        self.lin3gauss_logvar = nn.Linear(N, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.relu(x)\n",
    "        xgauss_mean = self.lin3gauss_mean(x)\n",
    "        xgauss_logvar = self.lin3gauss_logvar(x)\n",
    "\n",
    "        return xgauss_mean, xgauss_logvar\n",
    "\n",
    "\n",
    "# Decoder\n",
    "class P_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(P_net, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        self.lin3 = nn.Linear(N, X_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# Utility functions\n",
    "###################\n",
    "\n",
    "\n",
    "def save_model(model, filename):\n",
    "    print('Best model so far, saving it...')\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "\n",
    "def report_loss(epoch, recon_loss):\n",
    "    '''\n",
    "    Print loss\n",
    "    '''\n",
    "    print('Epoch-{}; recon_loss: {:.4}'.format(epoch,recon_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Training procedure\n",
    "####################\n",
    "\n",
    "def train(P, Q, P_decoder, Q_encoder, data_loader):\n",
    "    '''\n",
    "    Train procedure for one epoch.\n",
    "    '''\n",
    "    TINY = 1e-15\n",
    "    # Set the networks in train mode (apply dropout when needed)\n",
    "    Q.train()\n",
    "    P.train()\n",
    "\n",
    "    # Loop through the labeled and unlabeled dataset getting one batch of samples from each\n",
    "    # The batch size has to be a divisor of the size of the dataset or it will return\n",
    "    # invalid samples\n",
    "    for it in range(100):\n",
    "\n",
    "        X, target = data_loader(train_batch_size)\n",
    "\n",
    "        # Load batch and normalize samples to be 0-mean and unit-std\n",
    "        X = (X + 4.55)/ 8.08\n",
    "        X.resize_(train_batch_size, X_dim)\n",
    "        X, target = Variable(X), Variable(target)\n",
    "        if cuda:\n",
    "            X, target = X.cuda(), target.cuda()\n",
    "\n",
    "        # Init gradients\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "\n",
    "\n",
    "        # Reconstruction phase\n",
    "\n",
    "        z_mean, z_logvar = Q(X)\n",
    "        std = z_logvar.mul(0.5).exp_()\n",
    "        if cuda:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        z_sample = eps.mul(std).add_(z_mean)\n",
    "\n",
    "        X_sample = P(z_sample)\n",
    "        criterion = nn.MSELoss()\n",
    "        criterion.size_average = False\n",
    "        recon_loss = criterion(X_sample, X.resize(train_batch_size, X_dim))\n",
    "\n",
    "        # -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD_element = z_mean.pow(2).add_(z_logvar.exp()).mul_(-1).add_(1).add_(z_logvar)\n",
    "        KLD_loss = torch.sum(KLD_element).mul_(-0.5)\n",
    "\n",
    "        loss =  recon_loss + KLD_loss\n",
    "\n",
    "        loss.backward()\n",
    "        P_decoder.step()\n",
    "        Q_encoder.step()\n",
    "\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "\n",
    "    return recon_loss\n",
    "\n",
    "\n",
    "def generate_model(train_labeled_loader, train_unlabeled_loader, valid_loader):\n",
    "    torch.manual_seed(10)\n",
    "\n",
    "    if cuda:\n",
    "        Q = Q_net().cuda()\n",
    "        P = P_net().cuda()\n",
    "    else:\n",
    "        Q = Q_net()\n",
    "        P = P_net()\n",
    "\n",
    "    # Set learning rates\n",
    "    gen_lr = 0.0001\n",
    "\n",
    "    # Set optimizators\n",
    "    P_decoder = optim.Adam(P.parameters(), lr=gen_lr)\n",
    "    Q_encoder = optim.Adam(Q.parameters(), lr=gen_lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #x,dummy = train_unlabeled_loader(train_batch_size)\n",
    "        recon_loss = train(P, Q, P_decoder, Q_encoder,\n",
    "                                                 train_unlabeled_loader)\n",
    "        if epoch % 10 == 0:\n",
    "            report_loss(epoch, recon_loss)\n",
    "\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Train a generative model\n",
    "##########################\n",
    "\n",
    "Q, P = generate_model(data_loader_Cdist, data_loader_Cdist, data_loader_Cdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "# Visualize generated distribution\n",
    "##################################\n",
    "\n",
    "def generate_Cdist(numSamples):\n",
    "\n",
    "    z = np.random.randn(numSamples,2).astype(np.float32)        # Sample from Gaussian distribution\n",
    "\n",
    "    z = Variable(torch.from_numpy(z))\n",
    "    if cuda:\n",
    "        z = z.cuda()\n",
    "\n",
    "    x = P(z)\n",
    "\n",
    "    return x\n",
    "\n",
    "x = generate_Cdist(5000)\n",
    "x = np.array(x.data.tolist())\n",
    "x = x*8.08 - 4.55\n",
    "x1 = x[:,0]\n",
    "x2 = x[:,1]\n",
    "\n",
    "plt.scatter(x1,x2)\n",
    "plt.hold()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE seems to have done a near-decent job at modelling the C-shaped distribution. You can try and tune the VAE so that it models the distribution even better.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
